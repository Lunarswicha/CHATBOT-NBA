import os
import ollama
import streamlit as st
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

def load_vectorstore():
    """Charge les embeddings FAISS."""
    if not os.path.exists("data/nba_knowledge"):
        raise FileNotFoundError("üö® Embeddings FAISS introuvables. Ex√©cute `preprocessing.py` d'abord !")

    embeddings_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L12-v2")
    return FAISS.load_local("data/nba_knowledge", embeddings_model, allow_dangerous_deserialization=True)

def save_new_knowledge(query, response):
    """Sauvegarde les nouvelles r√©ponses g√©n√©r√©es dans un fichier CSV."""
    file_path = "data/learned_knowledge.csv"
    
    # V√©rifie si le fichier existe, sinon le cr√©e avec un en-t√™te
    if not os.path.exists(file_path):
        with open(file_path, "w", encoding="utf-8") as f:
            f.write("Question,R√©ponse\n")

    # Ajoute la nouvelle question-r√©ponse
    with open(file_path, "a", encoding="utf-8") as f:
        f.write(f"\"{query}\",\"{response}\"\n")

    print("üíæ Nouvelle connaissance ajout√©e !")

def generate_response_with_ollama(query, retrieved_texts):
    """Utilise Ollama pour g√©n√©rer une r√©ponse en fran√ßais."""
    if not retrieved_texts:
        return "Je n'ai trouv√© aucune information pertinente pour r√©pondre √† cette question."

    context = "\n\n".join(retrieved_texts)
    prompt = f"""
    Contexte :
    {context}

    Question : {query}
    R√©ponds de mani√®re claire et concise en fran√ßais en t'appuyant uniquement sur les informations ci-dessus.
    """

    try:
        response = ollama.chat(model="mistral", messages=[{"role": "user", "content": prompt}])
        response_text = response["message"]["content"]
        save_new_knowledge(query, response_text)  # Sauvegarde la nouvelle connaissance
        return response_text
    except Exception as e:
        return f"‚ö†Ô∏è Erreur lors de l'appel √† Ollama : {e}"

def search_query(query, top_k=3):
    """Recherche les documents les plus pertinents via FAISS."""
    try:
        vectorstore = load_vectorstore()
        results = vectorstore.similarity_search_with_score(query, k=top_k)
        
        # Prioriser les nouvelles connaissances
        results = sorted(results, key=lambda x: ("learned_knowledge.csv" in x[0].metadata.get("source", ""), x[1]), reverse=True)
    except Exception as e:
        st.error(f"‚ö†Ô∏è Erreur lors de la recherche : {e}")
        return []

    return [doc.page_content for doc, score in results if doc.page_content.strip()]

def main():
    st.title("üèÄ B-BALL")
    st.write("Pose une question sur la NBA et obtiens une r√©ponse intelligente qui s'am√©liore avec le temps !")
    
    query = st.text_input("üîé Pose ta question sur la NBA :")
    
    if st.button("Envoyer"):
        if not query:
            st.warning("‚ö†Ô∏è Veuillez entrer une question valide !")
        else:
            results = search_query(query)
            retrieved_texts = [content for content in results]
            final_response = generate_response_with_ollama(query, retrieved_texts)
            
            st.subheader("üéØ R√©ponse g√©n√©r√©e :")
            st.write(final_response)

if __name__ == "__main__":
    main()
